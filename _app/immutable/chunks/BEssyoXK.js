import{f,a as g}from"./C7KGA84w.js";import"./BbVGT5Nm.js";import{h as V,J as G,$ as U,aw as x,K as n,M as _e,O as e,P as t}from"./gYfsknW5.js";import{h as Y}from"./DhCbiuj4.js";import{a as o}from"./CMntdO6A.js";function Ve(i,r,m){var s=i==null?"":""+i;return s===""?null:s}function Ge(i,r){return i==null?null:String(i)}function a(i,r,m,s,h,l){var p=i.__className;if(V||p!==m||p===void 0){var d=Ve(m);(!V||d!==i.getAttribute("class"))&&(d==null?i.removeAttribute("class"):i.className=d),i.__className=m}return l}function c(i,r,m,s){var h=i.__style;if(V||h!==r){var l=Ge(r);(!V||l!==i.getAttribute("style"))&&(l==null?i.removeAttribute("style"):i.style.cssText=l),i.__style=r}return s}const Ue={date:"2026-01-01",description:"Thoughts on what LLM's can produce.",title:"On the Epistemic Status of Generative Outputs",meta:[{name:"description",content:"Thoughts on what LLM's can produce."}]};var Ye=f(`<meta name="description" content="Thoughts on what LLM's can produce."/>`),Qe=f("<p>I keep coming back to the same question whenever I work with large language models: <em>what, exactly, is the epistemic status of what they produce?</em> Not whether it is useful, or convincing—but whether it should be understood as knowledge in any meaningful philosophical sense. This question matters, because we increasingly treat generative outputs as if they participate in the same epistemic economy as human claims. They are cited, trusted, debated, and sometimes acted upon. If we don’t clarify what kind of thing an LLM output actually is, we risk confusing fluency with truth.</p> <p>At a baseline, I think we need a conditional notion of objectivity for generative systems. An output from an LLM can be considered objectively true <strong>if and only if</strong> it satisfies the linguistic, factual, and societally accepted standards of truth within a particular domain. The condition is strict, and intentionally so. The truth does not originate in the model; it is inherited from an external epistemic framework that predates and constrains the model’s training.</p> <p>Consider a simple example. If an LLM states that water is composed of two hydrogen atoms and one oxygen atom, we are comfortable labeling that statement objectively true. But the truth of the statement has nothing to do with the internal mechanics of the model. It is true because it coheres with a well-established scientific ontology, one that has been experimentally validated, pedagogically transmitted, and socially integrated. The model is not discovering this fact; it is reproducing a consensus that already exists. In this sense, the model functions as a high-dimensional retrieval and rearticulation mechanism, not as an epistemic agent.</p> <p>The picture changes immediately when we move from reproduction to novelty. When an LLM generates a new hypothesis, a novel explanation, or an unfamiliar theoretical structure, the epistemic status of that output is fundamentally different. Such an output is not objectively true by default, regardless of how plausible or well-formed it appears. It becomes truth-apt only if it is subjected to external validation—evaluation by domain experts, empirical testing where applicable, and eventual legitimation by the relevant intellectual community. Until that process occurs, the output remains a linguistic artifact that <em>resembles</em> knowledge without yet being verified as such.</p> <p>This distinction leads to an important but often blurred conclusion: large language models are capable of producing objective truths, but they are not capable of originating them on their own. The truths they generate are parasitic on prior human epistemic labor. This their fundamental training and inferential constraint. Treating LLMs as independent discoverers of truth confuses the surface form of knowledge with the conditions under which knowledge is produced.</p> <p>At this point, the discussion naturally turns to probability. Every token an LLM outputs is selected because it is, given the model’s training and context window, the highest-probability linguistic continuation. This raises a deeper philosophical question: if an output is probabilistically optimal in linguistic space, does that confer any claim to objective truth? In other words, is the model’s notion of “likelihood” aligned with the human notion of “truth”?</p> <p>I think the answer is no not in any robust sense. LLMs are grounded in linguistic reality, not ontological reality. Language is not a transparent window onto the world. It is a representational system shaped by convention, necessary information compression, and historical relevance. It allows humans to coordinate, reason, and infer, but it does not exhaust the structure of the things it describes. Linguistic coherence is therefore a necessary but insufficient condition for truth.</p> <p>Because of this, LLM outputs do not directly track ontological truth. They approximate it by reproducing the statistical contours of how humans talk about the world. When an LLM appears to “understand” something, what it is actually doing is navigating a space of linguistic regularities that correlate—sometimes very strongly—with human knowledge. The correlation can be extremely useful, but it is still a correlation. The model does not have access to the substrate that gives those regularities their truth conditions.</p> <p>This limitation becomes especially clear when we talk about discovery. LLM-based systems can absolutely contribute to applied research. They can surface connections, generate candidate ideas, and assist humans in exploring large conceptual spaces more efficiently. But discovering a new ontological truth—introducing a genuinely new mathematical object, proving a theorem that reorganizes an existing field, or formulating a theory that reshapes our understanding of some domain—requires grounding beyond language alone. When such discoveries occur with LLMs in the loop, they are better understood as cases of human-guided exploration aided by stochastic recombination, not as autonomous epistemic achievements by the model. I like the example of Terrance Tao using copilot to formalize a proof. He laid the groundwork and the LLM sped up the follow-through.</p> <p>In that sense, the generation of new ontology by an LLM is largely a matter of chance filtered through human judgment. The model can propose. Humans must dispose. Recognizing this boundary is not an attempt to diminish the power of generative systems. On the contrary, it makes their role clearer. LLMs are extraordinarily capable linguistic instruments, but they do not bear the burden of truth on their own. That burden remains with the human communities that define, test, and sustain what counts as knowledge in the first place.</p>",1);function Ze(i){var r=Qe();Y("1nat5d2",m=>{var s=Ye();G(()=>{U.title="On the Epistemic Status of Generative Outputs"}),g(m,s)}),x(18),g(i,r)}const et=Object.freeze(Object.defineProperty({__proto__:null,default:Ze,metadata:Ue},Symbol.toStringTag,{value:"Module"})),tt={date:"2025-05-04",description:"My East Asian Language Final Project on Language Acquisition",title:"Analysis of Manner of Motion Verbs in Korean Heritage Learners",meta:[{name:"description",content:"My East Asian Language Final Project on Language Acquisition"}]};var at=f('<meta name="description" content="My East Asian Language Final Project on Language Acquisition"/>'),nt=f("<h2>Introduction:</h2> <p>Throughout Languages of East Asia, we have learned about the different linguistic frameworks for the major east asian languages—Chinese, Japanese, and Korean. In particular, we focused on the way in which the encoding of meaning differs based on the semantic means of each language. Additionally, we learned about how each language uses different semantic means to contribute to meaning like tones in Chinese or pitch patterns in specific Japanese dialects. As a result, we briefly touched upon the cognitive typologies in Chinese, Japanese, and Korean but without much discussion around how various typological structures affect acquisition of each language.</p> <p>In preparation for this project, Professor Kamiya suggested <em>“How Cognitive Typology Affects Second Language Acquisition: A Study of Japanese and Chinese Learners of English”</em> by Ryan Spring and Kaoru Horie as the basis for the larger research question. This paper details an observational study in which learners of English describe what is happening in various video clips to show how the manner of motion framing of one's first languages affects their tendencies when learning a different language with different framing.</p> <p>For my final project, I chose to replicate the same study on the manner-of-motion verbs but in reverse and with American-born heritage learners of Korean. To this end, I interviewed three students to investigate whether Korean heritage learners differ from non-heritage learners in their use of manner-of-motion verbs, predicting that heritage exposure will reduce reliance on English-like “go + preposition” constructions.</p> <hr/> <h2>Theoretical and Empirical Background:</h2> <p>The key characteristic of Korean we will focus on is how two events can be conflated into a single macro event. More specifically, conflation refers to how a language like Korean encodes the main event onto the main verb of a sentence. Spring and Horie state that languages are either “verb-framed,” “satellite-framed,” or “equipollently-framed,” which is a both of the previous two. Verb-framed languages encode the path of the event onto the verb and conflate the manner of motion onto other particles. Spanish is an example of a verb-framed language. The example Spring and Horie use is,</p> <blockquote><p>“La bollata entró a la cueva flotando.”</p> <p><em>The bottle moved.in to the cave floating.</em></p></blockquote> <p>Here, the manner of the motion is encoded onto the adverb “floating,” and the particle on “entró” denotes that the path of the bottle is moving in. On the other hand, satellite-framed languages encode the path onto the preposition and conflate the manner of motion onto the main verb. For example in English (a satellite-framed language),</p> <blockquote><p>“The car slowly parked in the garage”</p></blockquote> <p>encodes the path of the parking on the preposition “in” and the manner of the motion “slowly” on the verb “parked.” While these examples are relatively concrete, Spring and Horie note that Japanese has some exceptions where specific verbs can encode both path and manner. Furthermore, Chinese encodes both path and manner onto verbs which can make distinguishing the main verb difficult. Thus, Spring and Horie defined equipollently-framed languages to show that Chinese verbs can hold both meanings.</p> <p>As it turns out, Korean is also a verb-framed language as stated in <em>“Language-specificity of motion event expressions in young Korean children”</em> by Soonja Choi. It is reasonable to suspect that native speakers of Korean would avoid using manner-of-motion verbs since they are often preoccupied with encoding the path of the main event. I suspect that native Korean speakers learning English would have a tendency to port “go + preposition” in the place of path verbs from their native language, because Spring and Horie observed this substitution with their Japanese learners of English. That being said, they did not study the reverse of verb-framed language speakers learning satellite-framed languages.</p> <hr/> <h2>Research Question and Hypothesis:</h2> <p>As a Korean-American with some exposure to Korean at home, I was naturally curious if my learning experience would be different from someone learning Korean without any heritage or prior experiences with the language. More specifically, I want to address the question of whether or not heritage experience affects the tendency of learners of Korean to refer to the path and manner of motion encoding framework of native English speakers. Thus, I hypothesize that regardless of heritage exposure, the interviewees will steer towards verbs with more direct path encodings and rely more on adverbs to show manner of motion with an increase toward Korean-specific verbs as proficiency rises. However, low-proficiency learners will continue to use the “go + preposition” style, likely as a result of limited vocabulary and because Korean is a verb-framed language and the participants are native satellite-framed language speakers.</p> <hr/> <h2>Methodologies and Results:</h2> <p>To best minimize the differences between tests run by Spring and Horie and the tests for this study, I interviewed three Korean-American Hamilton College students of varying levels of Proficiency. Student 1 and Student 2 described themselves as able to hold simple conversations with family or in casual settings while studying abroad in Korea. However, Student 3 speaks Korean as a primary language at home. He is currently a Korean 4 student at Hamilton. Thus, I categorized Student 1 and Student 2 as low proficiency and Student 3 as highly proficient.</p> <p>Each student was shown a sampling of the images provided in Spring and Horie’s results section and asked to write down what they interpreted from the scene in Hangul. In order to stimulate authentic word choice, students were not informed of the Spring and Horie interpretations.</p> <p>The motion events and translations for each student are <strong>bolded below</strong>. The <em>italicized translations</em> are the descriptions given by Spring and Horie while the non-italicized translations are the more literal, formatted translations of what the Korean heritage learners said in Korean.</p> <hr/> <p><strong>The boy leaves his apartment.</strong></p> <p><strong>The boy goes down the stairs.</strong></p> <p><strong>The boy leaves the room.</strong></p> <p><strong>The boy rides a bicycle to the swimming pool.</strong></p> <p><strong>The boy follows the boy who is riding a bicycle.</strong></p> <hr/> <h3>Student 1 (S1)</h3> <p>남자가 아파트어 나가요</p> <blockquote><p>“The man go.out from the apartment.”</p></blockquote> <p><em>The boy leaves his apartment.</em></p> <p>수영장에 bicycle 타여</p> <blockquote><p>“He ride bicycle to the swimming-pool.”</p></blockquote> <p><em>The boy rides a bicycle to the swimming pool.</em></p> <hr/> <h3>Student 2 (S2)</h3> <p>소년이 방을 떠나요.</p> <blockquote><p>“The boy leave the room.”</p></blockquote> <p><em>The boy leaves the room.</em></p> <p>남자는 계단을 밑에서.</p> <blockquote><p>“The man go.down the stairs.”</p></blockquote> <p><em>The boy goes down the stairs.</em></p> <hr/> <h3>Student 3 (S3)</h3> <p>아이가 게단을 내려가요.</p> <blockquote><p>“The child go.down the stairs.”</p></blockquote> <p><em>The boy goes down the stairs.</em></p> <p>아이가 자장거를 타는 에를 따라가요.</p> <blockquote><p>“The child follow the kid ride bicycle.”</p></blockquote> <p><em>The boy follows the boy who is riding a bicycle.</em></p> <p>아이가 집을 나가고 있어요.</p> <blockquote><p>“The child be.going.out from the house.”</p></blockquote> <p><em>The boy leaves his apartment.</em></p> <hr/> <p>These structured (non-italicized) translations were created in accordance with the National Institute of Korean Languages’s Korean-English Learner's Dictionary. The tendencies of each participant will be discussed in the following section where each student is referred to as S1, S2, and S3 respectively. Each example will be referred to as S1 no.1 for example.</p> <hr/> <h2>Analysis and Discussion of Results:</h2> <p>Before beginning analysis of verb use, note that the subject nouns vary from case to case, but this is to be expected as the students were not informed of what/who the subject is in each image, and this did not affect verb choice since the chosen subject nouns were all generalized.</p> <p>By observation, the most noticeable difference between the responses from S3 and those from S1 and S2 is the extended richness of S3’s path verbs. In other words, the main verbs in the responses from S3 deviates from simply using “go + location,” which occurs in half the responses from S2 and S1. Specifically, S3’s use of 나가고 있(be.going.out) roughly adds the auxiliary “be,” verb to “going.out,” with the character “있,” which indicates that the subject is not merely going out, but rather is in a state of going out. This verb choice is more inline with the specific notion of “leaving,” and it includes a Korean-specific word that is without an equivalent in English. This directly supports the hypothesis. Furthermore, S3 uses “타는,” (riding) as a descriptive verb in no. 2, illustrating a much richer verb form compared to S2 and S1, while in example no. 3, using “go,” as the main verb is the correct choice.</p> <p>The superior correctness and specificity of path verbs from S3 demonstrates not only the strong diction of a more proficient learner, but also the contextual understanding of when to illustrate the manner of motion and path in the same verb. The significance of this shift is that S3 shows a correlation between proficiency and accuracy of the Korean verb-framework. This most directly supports the claim that as proficiency increases, the use of more explicit path verbs in place of “go + preposition,” increases as well.</p> <p>You may notice that of all the examples, only one (S3 no. 2) contains a relational clause. This was not done on purpose, but because S1 and S2 simply did not know how to translate their interpretation of the images from English to Korean. This is why their responses were not included. However, some interesting implications arise when examining the lexical detail of S3’s verb choice.</p> <p>Referring to the literal translation of S3 no. 2, “따라 + 가요,” is a compound verb meaning “follow; go after” instead of the generic “따라, to follow,” according to the Korean-English Learners Dictionary. This further shows a depth of complexity that was not quite learned by S1 and S2. In particular, the inclusion of the relative form for follow indicates that the path encoded onto the verb has evolved past simple prepositions. In fact, S3 includes a path that is integrated into the verb, but also a path that refers to another moving object. Not only does S3 show a preference toward direct-path verbs, but also a comfort with including more complex ideas into the path of the main event, and this also supports the previous claim.</p> <p>An unexpected result of these interviews is the complete lack of manner-of-motion identifiers around the main verb. This could be accounted for by the fact that each student was only shown the static images from the paper, but also the lack of vocabulary in S1 and S2. “Consistency of Motion Event Encoding Across Languages” by Guillermo Montero-Melis may point to why S3 also did not include manner of motion. Monter-Melis shows that verb-framed language users are about 34% less likely to include manner of motion elements compared to satellite-framed users, such as the Swedish subjects in the study. So as a highly proficient speaker, S3 may have excluded manner of motion in compliance with standard Korean language practice.</p> <p>Ultimately, the general lack of manner of motion elements may conflict with the claim that in general, heritage speakers would rely on adverbs to convey manner of motion. However, further study may be needed to say for certain that advanced speakers would or would not use adverbs instead of more specific verbs to show manner of motion. As it stands right now, I believe that as proficiency and vocabulary size increase, highly proficient users still gravitate towards more detailed verbs, but because of the conventions of Korean, less so than relying less on the satellite-framed structure of English.</p> <p>When analyzing the results of the interviews, it is important to note that the range of proficiency was very wide with a low number of participants due to time constraints. This caused difficulties when analyzing patterns involving manner of motion since the speakers were either too novice to give more than simple sentences or so advanced that they would adhere to the actual conventions of the language. Gathering data from intermediate level learners could have provided more insight into the ways that heritage learners encode manner of motion. This appeared to be less of an issue when discussing path encodings, because the main verb is a required component of a complete sentence. Generally speaking, all results could benefit from a larger, more diverse test group.</p> <hr/> <h2>Discussion about Heritage Learners:</h2> <p>The initial goal of this study was to explore the results produced by Spring and Horie with heritage learners to observe whether or not their findings hold true in a reverse scenario and if heritage exposure makes a difference in the tendencies of students. Overall, observations from analyzing the results support the subclaim that as proficiency increases, heritage learners of Korean will increasingly use more direct path verbs with higher accuracy. However, it was discussed that observations of how heritage learners encode manner of motion were largely inconclusive. This is likely due to a general lack of vocabulary in lower level learners, since speakers of satellite-framed languages capture manner of motion almost 100% of the time according to Montero-Melis.</p> <p>Of the three students, S3 is the only one that speaks Korean as the primary language at home. He also has the most formal education in Korean. It is possible that his overall proficiency and knowledge of standard Korean contributed to the lack of manner-of-motion elements in his responses, but it may also be the case that S3 grew up learning both verb-framing and satellite-framing equally and can easily code switch between them. A larger study with a control group of intermediate speakers who only learned Korean at home may provide some clarity to these questions.</p> <p>When initially forming my hypothesis, I intuitively thought that heritage speakers may have an easier time adjusting to a verb-framed language due to past exposure at a younger age. However, after reflecting on my own experiences, I realized that the majority of the vocabulary I was taught were nouns and polite/honorific verb forms. Additionally, after speaking with the interviewees about their upbringing and being taught Korean, all three mentioned that they feel heritage speakers have a low level of proficiency in most of the United States. This could be due to a lack of Korean communities in many places or difficulties finding time to practice the language later in childhood and teenage years. I know this is true for myself: I stopped speaking Korean almost entirely after I went to kindergarten and entered a new primary social environment. That being said, each participant expressed keen interest in learning how standard Korean works and how to speak with fewer satellite-like structures, ultimately hinting at a deep cultural connection between the younger generation and their heritage culture.</p> <hr/> <h2>Works Cited</h2> <p>Choi, Soonja. “Language-Specificity of Motion Event Expressions in Young Korean Children.” <em>Language, Interaction and Acquisition</em>, vol. 2, no. 1, 2011, pp. 157-184. John Benjamins Publishing Company.</p> <p>Min, Jin-young, and Jean-myung Ahn. <em>Korean Grammar in Use: Intermediate.</em> Darakwon, 2011.</p> <p>Montero-Melis, Guillermo. “Consistency in Motion Event Encoding Across Languages.” <em>Frontiers in Psychology</em>, vol. 12, 2021, article 625153, doi:10.3389/fpsyg.2021.625153.</p> <p>National Institute of Korean Language. <em>Basic Korean Dictionary.</em> krdict.korean.go.kr. Accessed 6 May 2025.</p> <p>Spring, Ryan, and Kaoru Horie. “How Cognitive Typology Affects Second Language Acquisition: A Study of Japanese and Chinese Learners of English.” <em>Cognitive Linguistics</em>, vol. 24, no. 4, 2013, pp. 689-710, doi:10.1515/cog-2013-0024.</p>",1);function it(i){var r=nt();Y("19r6dzd",m=>{var s=at();G(()=>{U.title="Analysis of Manner of Motion Verbs in Korean Heritage Learners"}),g(m,s)}),x(152),g(i,r)}const ot=Object.freeze(Object.defineProperty({__proto__:null,default:it,metadata:tt},Symbol.toStringTag,{value:"Module"})),st={date:"2025-01-08",description:"Writing down the differences between latent space and embedding space",title:"Distinguishing Latent Space, and Embedding Space",meta:[{name:"description",content:"Writing down the differences between latent space and embedding space"}]};var rt=f('<meta name="description" content="Writing down the differences between latent space and embedding space"/> <link rel="stylesheet" href="/sveltex/mathjax@3.2.2.chtml.min.css"/>',1),mt=f(`<h2>Introduction</h2> <p>For a while I've had trouble really understanding the difference between latent space,
embedding space, their elements and contexts in which to write and discuss about them. Today,
I'll try to elucidate the differences and similarities primarily in the context of autoregressive transformer models.</p> <h2>Primer</h2> <p>The common example used when defining latent space is the Variational Autoencoder (VAE). VAE’s are generative models that aim to create new data from the variations of what it is trained on. While most autoencoders learn a discrete representation of latent variables, the VAE learns a continuous probabilistic latent space, hence its favorability as an example in latent space discussions. The VAE architecture also implies the reality of its latent space as a low dimensional internal representation space meaning that (mathematically) it has some structure about the latent variables. A latent space is not necessarily algebraic as we will see later, but understanding it is crucial since every hidden model has a latent space from VAE, to Hidden Markov Chains, to autoregressive LLMs as we will see next.</p> <p>Let’s take a step back and talk about Transformer based language models and in particular the nature of embeddings. If we look at the architecture of a Transformer, we see the encoder and before that the embedding layer. This layer is a learnable mapping that takes the discrete token sequence and projects it into a high-dimensional euclidean vector space <mjx-container><mjx-math><mjx-msup><mjx-TeXAtom><mjx-mi><mjx-c></mjx-c></mjx-mi></mjx-TeXAtom><mjx-script><mjx-TeXAtom><mjx-mi><mjx-c></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msup></mjx-math></mjx-container>. This vector space is the embedding space and for now that’s all we need. In other words, it is the coordinate system in which embeddings contain semantic meaning as learned by the embedding layer.</p> <h2>Discussion of Spaces</h2> <p>Now that we have our embeddings as high-dimensional vectors in the embedding space, they are passed through the encoder and transformed in the process. This transformation captures whole-input semantic meaning that is then used in the decoder layers to provide context to further inputs of past output embeddings. More importantly, this transformation does not change the structure and size of the vectors such that it may no longer qualify as an element of the embedding space, rather it changes its semantic meaning entirely.</p> <p>Going back to the definition of latent spaces from before, let’s describe its elements as latent vectors or vectors that contain hidden representation and satisfy inclusion in the latent space. This is where the previous definition of the latent space becomes important. Since our embedding vectors have now transformed but still retain hidden representation, they are now latent vectors. Well, that’s not exactly right. They have always been latent vectors of the embedding space! That’s the big takeaway. What the encoder does is transform the embedding vectors into different latent vectors in the encoder and decoder latent space which allows for the self-attended autoregression to occur.</p> <h2>In Short</h2> <p>The latent space is the more general internal representation space that only depends on what the model needs to learn the data.</p> <p>The embedding space is an initial latent space that serves as a semantic coordinate system for the embedding vectors. It is also often a vector space.</p> <p>All models that learn hidden representations have a latent space but these can vary widely from vector spaces to manifolds to probabilistic spaces or whatever the model and its internal operations need.</p>`,3);function ct(i){var r=mt();Y("24boe1",Z=>{var _=rt();x(2),G(()=>{U.title="Distinguishing Latent Space, and Embedding Space"}),g(Z,_)});var m=n(_e(r),8),s=n(e(m));a(s,1,"MathJax"),o(s,"jax","CHTML");var h=e(s);a(h,1," MJX-TEX");var l=e(h),p=e(l);o(p,"texclass","ORD");var d=e(p);a(d,1,"mjx-ds mjx-b");var Q=e(d);a(Q,1,"mjx-c211D TEX-A"),t(d),t(p);var u=n(p);c(u,"vertical-align: 0.363em;");var j=e(u);o(j,"size","s"),o(j,"texclass","ORD");var v=e(j);a(v,1,"mjx-i");var b=e(v);a(b,1,"mjx-c1D451 TEX-I"),t(v),t(j),t(u),t(l),t(h),t(s),x(),t(m),x(14),g(i,r)}const ht=Object.freeze(Object.defineProperty({__proto__:null,default:ct,metadata:st},Symbol.toStringTag,{value:"Module"})),lt={date:"2026-01-01",description:"Trying LaTeX with markdown for clean math notes.",title:"Notes on Math and Markdown",meta:[{name:"description",content:"Trying LaTeX with markdown for clean math notes."}]};var dt=f('<meta name="description" content="Trying LaTeX with markdown for clean math notes."/> <link rel="stylesheet" href="/sveltex/mathjax@3.2.2.chtml.min.css"/>',1),pt=f("<p>Inline math looks good when it stays brief, like <mjx-container><mjx-math><mjx-mi><mjx-c></mjx-c></mjx-mi><mjx-mo><mjx-c></mjx-c></mjx-mo><mjx-mi><mjx-c></mjx-c></mjx-mi><mjx-msup><mjx-mi><mjx-c></mjx-c></mjx-mi><mjx-script><mjx-mn><mjx-c></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container>.</p> <p>Block math should be given proper space as I was taught in undergrad.</p> <p>E.g.</p> <mjx-container><mjx-math><mjx-msubsup><mjx-mo><mjx-c></mjx-c></mjx-mo><mjx-script><mjx-TeXAtom><mjx-mi><mjx-c></mjx-c></mjx-mi></mjx-TeXAtom><mjx-spacer></mjx-spacer><mjx-TeXAtom><mjx-mo><mjx-c></mjx-c></mjx-mo><mjx-mi><mjx-c></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msubsup><mjx-msup><mjx-mi><mjx-c></mjx-c></mjx-mi><mjx-script><mjx-TeXAtom><mjx-mo><mjx-c></mjx-c></mjx-mo><mjx-msup><mjx-mi><mjx-c></mjx-c></mjx-mi><mjx-script><mjx-mn><mjx-c></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-TeXAtom></mjx-script></mjx-msup><mjx-mstyle><mjx-mspace></mjx-mspace></mjx-mstyle><mjx-mi><mjx-c></mjx-c></mjx-mi><mjx-mi><mjx-c></mjx-c></mjx-mi><mjx-mo><mjx-c></mjx-c></mjx-mo><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo><mjx-c></mjx-c></mjx-mo></mjx-surd><mjx-box><mjx-mi><mjx-c></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container> <p>A quick matrix:</p> <mjx-container><mjx-math><mjx-mrow><mjx-mo><mjx-c></mjx-c></mjx-mo><mjx-mtable><mjx-table><mjx-itable><mjx-mtr><mjx-mtd><mjx-mn><mjx-c></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd><mjx-mtd><mjx-mn><mjx-c></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mtr><mjx-mtr><mjx-mtd><mjx-mn><mjx-c></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd><mjx-mtd><mjx-mn><mjx-c></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-table></mjx-mtable><mjx-mo><mjx-c></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container>",3);function ut(i){var r=pt();Y("14syp1r",Ne=>{var Fe=dt();x(2),G(()=>{U.title="Notes on Math and Markdown"}),g(Ne,Fe)});var m=_e(r),s=n(e(m));a(s,1,"MathJax"),o(s,"jax","CHTML");var h=e(s);a(h,1," MJX-TEX");var l=e(h);a(l,1,"mjx-i");var p=e(l);a(p,1,"mjx-c1D438 TEX-I"),t(l);var d=n(l);a(d,1,"mjx-n"),o(d,"space","4");var Q=e(d);a(Q,1,"mjx-c3D"),t(d);var u=n(d);a(u,1,"mjx-i"),o(u,"space","4");var j=e(u);a(j,1,"mjx-c1D45A TEX-I"),t(u);var v=n(u),b=e(v);a(b,1,"mjx-i");var Z=e(b);a(Z,1,"mjx-c1D450 TEX-I"),t(b);var _=n(b);c(_,"vertical-align: 0.363em;");var A=e(_);a(A,1,"mjx-n"),o(A,"size","s");var ke=e(A);a(ke,1,"mjx-c32"),t(A),t(_),t(v),t(h),t(s),x(),t(m);var y=n(m,6);a(y,1,"MathJax"),o(y,"jax","CHTML"),o(y,"display","true");var T=e(y);o(T,"display","true"),c(T,"margin-left: 0; margin-right: 0;"),a(T,1," MJX-TEX");var ee=e(T),M=e(ee);a(M,1,"mjx-lop");var Se=e(M);a(Se,1,"mjx-c222B TEX-S2"),t(M);var te=n(M);c(te,"vertical-align: -0.896em; margin-left: -0.388em;");var w=e(te);o(w,"size","s"),o(w,"texclass","ORD"),c(w,"margin-left: 0.647em;");var ae=e(w);a(ae,1,"mjx-n");var Le=e(ae);a(Le,1,"mjx-c221E"),t(ae),t(w);var fe=n(w);c(fe,"margin-top: 1.564em;");var I=n(fe);o(I,"size","s"),o(I,"texclass","ORD");var E=e(I);a(E,1,"mjx-n");var Ae=e(E);a(Ae,1,"mjx-c2212"),t(E);var ne=n(E);a(ne,1,"mjx-n");var Me=e(ne);a(Me,1,"mjx-c221E"),t(ne),t(I),t(te),t(ee);var K=n(ee);o(K,"space","2");var q=e(K);a(q,1,"mjx-i");var Ie=e(q);a(Ie,1,"mjx-c1D452 TEX-I"),t(q);var ie=n(q);c(ie,"vertical-align: 0.413em;");var X=e(ie);o(X,"size","s"),o(X,"texclass","ORD");var H=e(X);a(H,1,"mjx-n");var Ee=e(H);a(Ee,1,"mjx-c2212"),t(H);var xe=n(H),z=e(xe);a(z,1,"mjx-i");var Ke=e(z);a(Ke,1,"mjx-c1D465 TEX-I"),t(z);var oe=n(z);c(oe,"vertical-align: 0.363em;");var D=e(oe);a(D,1,"mjx-n"),o(D,"size","s");var qe=e(D);a(qe,1,"mjx-c32"),t(D),t(oe),t(xe),t(X),t(ie),t(K);var se=n(K),Xe=e(se);c(Xe,"width: 0.167em;"),t(se);var C=n(se);a(C,1,"mjx-i");var He=e(C);a(He,1,"mjx-c1D451 TEX-I"),t(C);var O=n(C);a(O,1,"mjx-i");var ze=e(O);a(ze,1,"mjx-c1D465 TEX-I"),t(O);var k=n(O);a(k,1,"mjx-n"),o(k,"space","4");var De=e(k);a(De,1,"mjx-c3D"),t(k);var re=n(k);o(re,"space","4");var je=e(re),me=e(je),ce=e(me);a(ce,1,"mjx-n");var Ce=e(ce);a(Ce,1,"mjx-c221A"),t(ce),t(me);var he=n(me);c(he,"padding-top: 0.334em;");var le=e(he);a(le,1,"mjx-i");var Oe=e(le);a(Oe,1,"mjx-c1D70B TEX-I"),t(le),t(he),t(je),t(re),t(T),t(y);var S=n(y,4);a(S,1,"MathJax"),o(S,"jax","CHTML"),o(S,"display","true");var L=e(S);o(L,"display","true"),c(L,"margin-left: 0; margin-right: 0;"),a(L,1," MJX-TEX");var ve=e(L),J=e(ve);a(J,1,"mjx-s3");var Je=e(J);a(Je,1,"mjx-c5B TEX-S3"),t(J);var B=n(J);c(B,"min-width: 2em;");var be=e(B),ye=e(be),de=e(ye),P=e(de);c(P,"padding-right: 0.5em; padding-bottom: 0.2em;");var W=e(P);a(W,1,"mjx-n");var Be=e(W);a(Be,1,"mjx-c31"),t(W),n(W),t(P);var pe=n(P);c(pe,"padding-left: 0.5em; padding-bottom: 0.2em;");var R=e(pe);a(R,1,"mjx-n");var Pe=e(R);a(Pe,1,"mjx-c30"),t(R),n(R),t(pe),t(de);var we=n(de),$=e(we);c($,"padding-right: 0.5em; padding-top: 0.2em;");var N=e($);a(N,1,"mjx-n");var We=e(N);a(We,1,"mjx-c30"),t(N),n(N),t($);var ue=n($);c(ue,"padding-left: 0.5em; padding-top: 0.2em;");var F=e(ue);a(F,1,"mjx-n");var Re=e(F);a(Re,1,"mjx-c31"),t(F),n(F),t(ue),t(we),t(ye),t(be),t(B);var ge=n(B);a(ge,1,"mjx-s3");var $e=e(ge);a($e,1,"mjx-c5D TEX-S3"),t(ge),t(ve),t(L),t(S),g(i,r)}const gt=Object.freeze(Object.defineProperty({__proto__:null,default:ut,metadata:lt},Symbol.toStringTag,{value:"Module"})),Te=Object.assign({"/src/content/posts/epistemic-llm.sveltex":et,"/src/content/posts/ksl-semantics.sveltex":ot,"/src/content/posts/latent-space-and-embeddings.sveltex":ht,"/src/content/posts/math-markdown.sveltex":gt}),ft=Object.entries(Te),xt=ft.map(i=>{const r=i[0],m=i[1],s=r.split("/").pop();let h="";return s&&(h=s.replace(".sveltex","")),{slug:h,title:m.metadata.title,date:m.metadata.date,description:m.metadata.description}});xt.sort((i,r)=>i.date<r.date?1:-1);const _t=i=>Te[`/src/content/posts/${i}.sveltex`];export{_t as a,xt as p};
