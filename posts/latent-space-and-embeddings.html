<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../_app/immutable/assets/0.BjdOO-NF.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.CIlouxhI.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/5pauIGJL.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/uxlmKjvO.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BUApaBEI.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Dvv8zL-s.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BcxZo3bI.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.DWFcwv2g.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B5gCWxvS.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Clhhf_VO.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/vXyCZIVp.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D_bPz67g.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D3tb3HLY.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_6vnAIgt.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.CLi0FzV4.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DPZMpzZM.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/MNuIXmfI.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DKubSo4n.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.B3rSkZoi.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CBbuBPfZ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Cd8CSMk1.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BGMjRVEo.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/a4ipC-2a.js"><!--12qhfyh--><link rel="stylesheet" href="../sveltex/mathjax@3.2.2.chtml.min.css"/> <link rel="icon" href="../favicon.svg"/><!----><!--14u6r3i--><!----><!--24boe1--><meta name="description" content="Writing down the differences between latent space and embedding space"/> <link rel="stylesheet" href="/sveltex/mathjax@3.2.2.chtml.min.css"/><!----><title>Distinguishing Latent Space, and Embedding Space</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><header><a href="../">Blog Home</a> <nav><ul><li><a href="../about">About Blog</a></li> <li><a target="_blank" rel="noopener noreferrer" href="https://alexmkim.io?skip=true">Landing Page</a></li></ul></nav></header><!----> <main><!----><article class="post"><h1>Distinguishing Latent Space, and Embedding Space</h1> <p class="post-meta">2026-01-08</p> <!----><h2>Introduction</h2> <p>For a while I've had trouble really understanding the difference between latent space,
embedding space, their elements and contexts in which to write and discuss about them. Today,
I'll try to elucidate the differences and similarities primarily in the context of autoregressive transformer models.</p> <h2>Primer</h2> <p>The common example used when defining latent space is the Variational Autoencoder (VAE). VAE’s are generative models that aim to create new data from the variations of what it is trained on. While most autoencoders learn a discrete representation of latent variables, the VAE learns a continuous probabilistic latent space, hence its favorability as an example in latent space discussions. The VAE architecture also implies the reality of its latent space as a low dimensional internal representation space meaning that (mathematically) it has some structure about the latent variables. A latent space is not necessarily algebraic as we will see later, but understanding it is crucial since every hidden model has a latent space from VAE, to Hidden Markov Chains, to autoregressive LLMs as we will see next.</p> <p>Let’s take a step back and talk about Transformer based language models and in particular the nature of embeddings. If we look at the architecture of a Transformer, we see the encoder and before that the embedding layer. This layer is a learnable mapping that takes the discrete token sequence and projects it into a high-dimensional Euclidean vector space <mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msup><mjx-TeXAtom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-TeXAtom><mjx-script style="vertical-align: 0.363em;"><mjx-TeXAtom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msup></mjx-math></mjx-container>. This vector space is the embedding space and for now that’s all we need. In other words, it is the coordinate system in which embeddings contain semantic meaning as learned by the embedding layer.</p> <h2>Discussion of Spaces</h2> <p>Now that we have our embeddings as high-dimensional vectors in the embedding space, they are transformed during inference. This transformation captures whole-input semantic meaning that is then used in the decoder layers to provide context to further inputs of past output embeddings. More importantly, this transformation does not change the structure and size of the vectors such that it may no longer qualify as an element of the embedding space, rather it changes its semantic meaning entirely.</p> <p>Going back to the definition of latent spaces from before, let’s describe its elements as latent vectors or vectors that contain hidden representation and satisfy inclusion in the latent space. This is where the previous definition of the latent space becomes important. Since our embedding vectors have now transformed but still retain hidden representation, they are now latent vectors. Well, that’s not exactly right. They have always been latent vectors of the embedding space! That’s the big takeaway. What the encoder does is transform the embedding vectors into different latent vectors in the encoder and decoder latent space which allows for the self-attended autoregression to occur.</p> <h2>In Short</h2> <p>The latent space is the more general internal representation space that only depends on what the model needs to learn the data.</p> <p>The embedding space is an initial latent space that serves as a semantic coordinate system for the embedding vectors. It is also often a vector space.</p> <p>All models that learn hidden representations have a latent space but these can vary widely from vector spaces to manifolds to probabilistic spaces or whatever the model and its internal operations need.</p><!----></article><!----><!----></main> <footer><p>Written by Alexander Kim</p> <p>Inspired by <a href="https://ashtyn.land">Ashtyn Morel-Blake</a></p></footer><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_18znv7a = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/idea_buckets"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.CIlouxhI.js"),
						import("../_app/immutable/entry/app.DWFcwv2g.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
